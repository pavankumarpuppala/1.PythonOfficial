{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing two types:\n",
    "- UNIT TESTING: The process of testing whether a particular unit is working properly or not is called unit testing\n",
    "    - it is developer responsible \n",
    "- Integration Testing: The process of testing total application(end to end testing)\n",
    "    - it is QA team reponsible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Scenario: single test Scenario contains multiple test cases\n",
    "#### Test Case: to test some functinality test case is written\n",
    "#### Test Suit: instead of executing individually i want to group all Test cases into one group is called Test Suite\n",
    "- execute that Test Suite Automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i've created a gmail application now i need to the application\n",
    "- testing login functionality --> Test Scenario\n",
    "    - valid username and valid password --> Test case1\n",
    "    - valid username and invalid password --> Test case2\n",
    "    - invalid username and valid password --> Test case3\n",
    "    - invalid username and invalid password --> Test case4\n",
    "    - null username and null password --> Test case5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Perform unittesting in python\n",
    "- module name: unittest\n",
    "- class name: Testcase\n",
    "- instance Methods: we have to overide these instance methodsin our Test class\n",
    "    1. setUp(): pre requisite activities to perform the testing(Ex. google.com should be open)\n",
    "    2. test()\n",
    "    3. tearDown(): once testing completed. whtaever environment created can you please clean that environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test (__main__.TestCaseDemo) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUp method execution\n",
      "***test method execution\n",
      "tearDown method execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x272ec7d95f8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCaseDemo(unittest.TestCase):\n",
    "    \n",
    "    # it should be same name\n",
    "    def setUp(self):\n",
    "        print('setUp method execution')\n",
    "        \n",
    "    # but we can change the test name and it should be prefix with test\n",
    "    def test(self):\n",
    "        print('***test method execution')  \n",
    "        \n",
    "    # it should be same name\n",
    "    def tearDown(self):\n",
    "        print('tearDown method execution')\n",
    "        \n",
    "        \n",
    "#unittest.main()  #normally   \n",
    "unittest.main(argv=[''], verbosity=2, exit=False) #juyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test (__main__.TestCaseDemo) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUp method execution\n",
      "***test method execution\n",
      "tearDown method execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test (__main__.TestCaseDemo)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-22-4068efcc3013>\", line 12, in test\n",
      "    print(10/0)\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x272ec7e1c88>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCaseDemo(unittest.TestCase):\n",
    "    \n",
    "    # it should be same name\n",
    "    def setUp(self):\n",
    "        print('setUp method execution')\n",
    "        \n",
    "    # but we can change the test name and it should be prefix with test\n",
    "    def test(self):\n",
    "        print('***test method execution')  \n",
    "        print(10/0)\n",
    "        \n",
    "    # it should be same name\n",
    "    def tearDown(self):\n",
    "        print('tearDown method execution')\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_method1 (__main__.TestCaseDemo) ... ok\n",
      "test_method2 (__main__.TestCaseDemo) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUp method execution\n",
      "***test method1 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method2 execution\n",
      "tearDown method execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x272ec7f2e80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCaseDemo(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        print('setUp method execution')\n",
    "        \n",
    "    def test_method1(self):\n",
    "        print('***test method1 execution') \n",
    "        \n",
    "    def test_method2(self):\n",
    "        print('***test method2 execution') \n",
    "        \n",
    "    def tearDown(self):\n",
    "        print('tearDown method execution')\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: for every test method execution before 1 setUp() and after 1 tearDown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setUp(): open chrome\n",
    "- test1(): test login functionality in google chrome\n",
    "- tearDown(): close chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setUp(): open firefox\n",
    "- test2(): test login functionality in firefox\n",
    "- tearDown(): close firefox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for all Test Cases a common functionality we have to do\n",
    "- setUpClass(cls)\n",
    "- test1(): test login functionality in google chrome with valid username and valid password\n",
    "- test2(): test login functionality in google chrome with valid username and invalid password\n",
    "- tearDownClass(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 test methods\n",
    "- setUp()  --> 10 times\n",
    "- tearDown() --> 10 times\n",
    "- setUpClass(cls) --> 1 time\n",
    "- tearDownClass(cls) --> 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_method1 (__main__.TestCaseDemo) ... ok\n",
      "test_method2 (__main__.TestCaseDemo) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUpClass method execution\n",
      "setUp method execution\n",
      "***test method1 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method2 execution\n",
      "tearDown method execution\n",
      "tearDownClass method execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x272ec7e95f8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCaseDemo(unittest.TestCase):\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print('setUpClass method execution')\n",
    "    \n",
    "    def setUp(self):\n",
    "        print('setUp method execution')\n",
    "        \n",
    "    def test_method1(self):\n",
    "        print('***test method1 execution') \n",
    "        \n",
    "    def test_method2(self):\n",
    "        print('***test method2 execution') \n",
    "        \n",
    "    def tearDown(self):\n",
    "        print('tearDown method execution')\n",
    "    \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print('tearDownClass method execution')\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_method1 (__main__.TestCaseDemo) ... ok\n",
      "test_method2 (__main__.TestCaseDemo) ... ERROR\n",
      "test_method3 (__main__.TestCaseDemo) ... ok\n",
      "test_method4 (__main__.TestCaseDemo) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUpClass method execution\n",
      "setUp method execution\n",
      "***test method1 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method2 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method3 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method4 execution\n",
      "tearDown method execution\n",
      "tearDownClass method execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_method2 (__main__.TestCaseDemo)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-26-ce7bd9cd0c57>\", line 17, in test_method2\n",
      "    print(10/0)\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.010s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x272ec803550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCaseDemo(unittest.TestCase):\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print('setUpClass method execution')\n",
    "    \n",
    "    def setUp(self):\n",
    "        print('setUp method execution')\n",
    "        \n",
    "    def test_method1(self):\n",
    "        print('***test method1 execution') \n",
    "        \n",
    "    def test_method2(self):\n",
    "        print('***test method2 execution')\n",
    "        print(10/0)\n",
    "        \n",
    "    def test_method3(self):\n",
    "        print('***test method3 execution')\n",
    "    \n",
    "    def test_method4(self):\n",
    "        print('***test method4 execution')\n",
    "        \n",
    "    def tearDown(self):\n",
    "        print('tearDown method execution')\n",
    "       \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print('tearDownClass method execution')\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automation Testing\n",
    "#### write a python script to test google search Functionality by using selenium unittesting\n",
    "- selenium : Functional Testing automation Tool\n",
    "    - package: selenium\n",
    "    - module: webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test (__main__.GoogleSearch) ... ERROR\n",
      "test_method1 (__main__.TestCaseDemo) ... ok\n",
      "test_method2 (__main__.TestCaseDemo) ... ERROR\n",
      "test_method3 (__main__.TestCaseDemo) ... ok\n",
      "test_method4 (__main__.TestCaseDemo) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUpClass method execution\n",
      "setUp method execution\n",
      "***test method1 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method2 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method3 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method4 execution\n",
      "tearDown method execution\n",
      "tearDownClass method execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "======================================================================\n",
      "ERROR: test (__main__.GoogleSearch)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-34-6fff1402821b>\", line 10, in setUp\n",
      "    driver = webdriver.chrome(executable_path='geckodriver.exe')\n",
      "TypeError: 'module' object is not callable\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_method2 (__main__.TestCaseDemo)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-26-ce7bd9cd0c57>\", line 17, in test_method2\n",
      "    print(10/0)\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.013s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x272eca73a90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = None\n",
    "class GoogleSearch(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        global driver\n",
    "        driver = webdriver.chrome(executable_path='geckodriver.exe')\n",
    "        driver.get('https://www.google.com/')\n",
    "        driver.maximize_window()\n",
    "        \n",
    "    def test(self):\n",
    "        driver.find_element_by_name('q').send_keys('pawan kalyan')\n",
    "        time.sleep(5)\n",
    "        driver.find_element_by_name('btnk').click()\n",
    "        driver.find_element_by_class_name('LC20lb').click()\n",
    "        \n",
    "    def tearDown(self):\n",
    "        time.sleep(10)\n",
    "        driver.close()\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- driver.title() : to get the title ofwindow\n",
    "- driver.current_url(): to get the current url\n",
    "- driver.refresh()\n",
    "- driver.get(driver.current_url)\n",
    "- driver.back(): goes one step back in the browser history\n",
    "- driver.forward(): goes one step forward in the browser history\n",
    "- driver.close() : it closes current window\n",
    "- driver.quit(): it closes all associated windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### order of test methods(Alphabetical order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_A (__main__.TestCaseDemo) ... ok\n",
      "test_B (__main__.TestCaseDemo) ... ok\n",
      "test_C (__main__.TestCaseDemo) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test A Method execution..\n",
      "Test B Method execution..\n",
      "Test C Method execution..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x2b364ceb940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCaseDemo(unittest.TestCase):\n",
    "    def test_C(self):\n",
    "        print(\"Test C Method execution..\")\n",
    "        \n",
    "    def test_B(self):\n",
    "        print(\"Test B Method execution..\")\n",
    "        \n",
    "    def test_A(self):\n",
    "        print(\"Test A Method execution..\")\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TestSuite: A group of testcases is called TestSuite\n",
    "- i want to execute below two test cases simulataneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUp method execution\n",
      "***test method1 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method2 execution\n",
      "tearDown method execution\n",
      "setUp method execution\n",
      "***test method1 execution\n",
      "tearDown method execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCaseDemo1(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        print('setUp method execution')\n",
    "        \n",
    "    def test_method1(self):\n",
    "        print('***test method1 execution') \n",
    "        \n",
    "    def test_method2(self):\n",
    "        print('***test method2 execution') \n",
    "        \n",
    "    def tearDown(self):\n",
    "        print('tearDown method execution')\n",
    "        \n",
    "class TestCaseDemo2(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        print('setUp method execution')\n",
    "        \n",
    "    def test_method1(self):\n",
    "        print('***test method1 execution') \n",
    "        \n",
    "    def tearDown(self):\n",
    "        print('tearDown method execution')\n",
    "        \n",
    "tc1 = unittest.TestLoader().loadTestsFromTestCase(TestCaseDemo1)\n",
    "tc2 = unittest.TestLoader().loadTestsFromTestCase(TestCaseDemo2)\n",
    "\n",
    "ts = unittest.TestSuite([tc1, tc2])\n",
    "unittest.TextTestRunner().run(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Assert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-21d09dcae275>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mAssert\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mAssert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mThat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Assert'"
     ]
    }
   ],
   "source": [
    "import Assert\n",
    "\n",
    "Assert.That(divide(10,2), Eq(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: C:\\Users\\LENOVO\\AppData\\Roaming\\jupyter\\runtime\\kernel-a52760f9-a6ae-4155-a5e3-ddcb6b1baf5c (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute 'C:\\Users\\LENOVO\\AppData\\Roaming\\jupyter\\runtime\\kernel-a52760f9-a6ae-4155-a5e3-ddcb6b1baf5c'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.016s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class LearningCase(unittest.TestCase):\n",
    "    def test_starting_out(self):\n",
    "        self.assertEqual(1, 1)\n",
    "\n",
    "def main():\n",
    "    unittest.main()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "def test_starting_out():\n",
    "    assert 1 == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of Unit Testing:\n",
    "- Test results will be displayed to the console only and it is not possible to generate reports\n",
    "- unittest framework always executes test methods in alphabetical order only and it is not possible to customize execute order\n",
    "- As the part of batch execution (TestSuite), all test methods from the specified TestCase classes will be executed and it is not possible to specify particular methods.\n",
    "- In unittesting only limited setUp and tearDown methods are available.\n",
    "    - setUpClass() --->Before executing all test methods of a TestCase class \n",
    "    - tearDownClass()-->After executing all test methods of a TestCase class \n",
    "    - setUp()-->Before every test method execution \n",
    "    - tearDown()-->After every test method execution\n",
    "- If we want to perform any activity before executing testsuite and after testsuite unittest framework does not define any methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Overcome above limitations we use PyTest\n",
    "- Advanced version of unittest framework\n",
    "- built on top of unittest framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytest naming conventions \n",
    "- Filename name starts or ends with 'test' Ex. test_google.py or google_test.py\n",
    "- class name should starts with 'Test' Ex. TestGoogleSearch\n",
    "- test method name should starts with 'Test_' Ex. test_method1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pytest_scripts/pytest_demo_test1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def test_methodA():\n",
    "    print(\"Test method A execution\")\n",
    "    \n",
    "def test_methodB():\n",
    "    print(\"Test method B execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pytest_scripts/pytest_demo_test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methodC():\n",
    "    print(\"Test method C execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.test  # it will executes all test scripts present in the pytest_scripts folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by defualt pytest won't display output in console to print output in console (py.test -s -v pytest_demo_test1.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to implement setUp() in pytest\n",
    "- using decorator @pytest.fixture()\n",
    "- @pytest.fixture() --> meant for setUp mechanism\n",
    "- @pytest.yield_fixture() --> meant for both setUp() and tearDown() \n",
    "- @pytest.yield_fixture(scope='module') --> setUPclass() and tearDownclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def setUp():\n",
    "    print('setup method execution')\n",
    "    \n",
    "def test_methodA():\n",
    "    print(\"Test method A execution\")\n",
    "    \n",
    "def test_methodB():\n",
    "    print(\"Test method B execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test method A execution<br>\n",
    "Test method B execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def setUp():\n",
    "    print('setup method execution')\n",
    "    \n",
    "def test_methodA(setUp):\n",
    "    print(\"Test method A execution\")\n",
    "    \n",
    "def test_methodB(setUp):\n",
    "    print(\"Test method B execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup method execution<br>\n",
    "Test method A execution<br>\n",
    "setup method execution<br>\n",
    "Test method B execution<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to implement tearDown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-fb072a37508b>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-fb072a37508b>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    setUp Activity\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@pytest.yield_fixture()\n",
    "def m1():\n",
    "    setUp Activity\n",
    "    yield\n",
    "    tearDown activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-b53be619abc6>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-b53be619abc6>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    tearDown activity\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@pytest.yield_fixture()\n",
    "def m1():\n",
    "    yield\n",
    "    tearDown activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.yield_fixture()\n",
    "def setUptearDown():\n",
    "    print('setup Activity')\n",
    "    yield\n",
    "    print('tearDown Activity')\n",
    "    \n",
    "def test_methodA(setUptearDown):\n",
    "    print(\"Test method A execution\")\n",
    "    \n",
    "def test_methodB(setUptearDown):\n",
    "    print(\"Test method B execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup Activity<br>\n",
    "Test method A execution<br>\n",
    "tearDown Activity<br>\n",
    "\n",
    "setup Activity<br>\n",
    "Test method B execution<br>\n",
    "tearDown Activity<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to implement setUpClass() nad tearDownClass() in pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.yield_fixture(scope='module')\n",
    "def m1():\n",
    "    print('setup class method Activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.yield_fixture(scope='module')\n",
    "def setUptearDownClass():\n",
    "    print('setupclass Activity')\n",
    "    yield\n",
    "    print('tearDownclass Activity')\n",
    "    \n",
    "def test_methodA(setUptearDownClass):\n",
    "    print(\"Test method A execution\")\n",
    "    \n",
    "def test_methodB(setUptearDownClass):\n",
    "    print(\"Test method B execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setupclass Activity<br>\n",
    "Test method A execution<br>\n",
    "Test method B execution<br>\n",
    "tearDownclass Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to implement setUp(), tearDown(), setUpClass() and tearDownClass() simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.yield_fixture(scope='module')\n",
    "def setUptearDownClass():\n",
    "    print('setupclass Activity')\n",
    "    yield\n",
    "    print('tearDownclass Activity')\n",
    "\n",
    "@pytest.yield_fixture()\n",
    "def setUptearDown():\n",
    "    print('setup Activity')\n",
    "    yield\n",
    "    print('tearDown Activity')\n",
    "    \n",
    "def test_methodA(setUptearDownClass, setUptearDown):\n",
    "    print(\"Test method A execution\")\n",
    "    \n",
    "def test_methodB(setUptearDownClass, setUptearDown):\n",
    "    print(\"Test method B execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setupclass Activity<br>\n",
    "setup Activity<br>\n",
    "Test method A execution<br>\n",
    "tearDown Activity<br>\n",
    "\n",
    "setup Activity<br>\n",
    "Test method B execution<br>\n",
    "tearDown Activity<br>\n",
    "tearDownclass Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: for multiple test scripts(test1.py, test2.py) we cannot write multiple setUp and tearDown methods. for this we have conftest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conftest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.yield_fixture(scope='module')\n",
    "def setUptearDownClass():\n",
    "    print('setupclass Activity')\n",
    "    yield\n",
    "    print('tearDownclass Activity')\n",
    "\n",
    "@pytest.yield_fixture()\n",
    "def setUptearDown():\n",
    "    print('setup Activity')\n",
    "    yield\n",
    "    print('tearDown Activity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methodA(setUptearDownClass, setUptearDown):\n",
    "    print(\"Test method A execution\")\n",
    "    \n",
    "def test_methodB(setUptearDownClass, setUptearDown):\n",
    "    print(\"Test method B execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methodC(setUptearDownClass, setUptearDown):\n",
    "    print(\"Test method C execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various possible ways to run pytest test scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- py.test -v -s --> to run all test methods present in all test scripts of cwd\n",
    "- py.test -v -s test1.py --> to run all test methods of a particular script\n",
    "- py.test -v -s test1.py test2.py\n",
    "- py.test -v -s test1.py::test_methodA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'wwebdriver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-91f077fb187a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpytest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mTestGoogleSearch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'wwebdriver'"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "from selenium import wwebdriver\n",
    "import time\n",
    "\n",
    "class TestGoogleSearch:\n",
    "    \n",
    "    @pytest.yield_fixture()\n",
    "    def setUptearDown(self):\n",
    "        global driver\n",
    "        driver = webdriver.chrome(executable_path='geckodriver.exe')\n",
    "        driver.get('https://www.google.com/')\n",
    "        driver.maximize_window()\n",
    "        yield\n",
    "        time.sleep(10)\n",
    "        driver.close()\n",
    "        \n",
    "    def test(self):\n",
    "        driver.find_element_by_name('q').send_keys('pawan kalyan')\n",
    "        time.sleep(5)\n",
    "        driver.find_element_by_name('btnk').click()\n",
    "        driver.find_element_by_class_name('LC20lb').click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
